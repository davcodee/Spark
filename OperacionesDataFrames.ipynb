{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import IntegerType, StringType,FloatType\n",
    "from pyspark.sql.types import Row\n",
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Juego DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"resources/files/\"\n",
    "juegoSchema = StructType([\n",
    "    StructField(\"juego_id\", IntegerType(), False),\n",
    "    StructField(\"anio\", StringType(), False),\n",
    "    StructField(\"Ciudad\",StringType(), False)\n",
    "])\n",
    "\n",
    "juegoDF = sqlContext.read.schema(juegoSchema) \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    . csv(path + \"juegos.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deportista Olimpico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "deportistaSchema = StructType([\n",
    "    StructField(\"deportista_id\", IntegerType(), False),\n",
    "    StructField(\"nombre\", StringType(), False),\n",
    "    StructField(\"genero\", IntegerType(), True),\n",
    "    StructField(\"edad\", IntegerType(),True), \n",
    "    StructField(\"altura\",IntegerType(), True),\n",
    "    StructField(\"peso\", FloatType(),True),\n",
    "    StructField(\"equipo_id\",IntegerType(), False),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "deportistaDF = sqlContext.read.schema(deportistaSchema) \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(path + \"deportista.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equipos Olimpicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESQUEMA\n",
    "paisesSchema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"equipo\", StringType(),True),\n",
    "    StructField(\"sigla\", StringType(),True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA FRAME\n",
    "paisesDF = sqlContext.read.schema(paisesSchema) \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(path + \"paises.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Resultados Olimpicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resultado_id,medalla,deportista_id,juego_id,evento_id\r\n",
      "1,NA,1,39,1\r\n",
      "2,NA,2,49,2\r\n",
      "3,NA,3,7,3\r\n",
      "4,Gold,4,2,4\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 5 resources/files/resultados.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCHEMA\n",
    "resultadosSchema = StructType([\n",
    "    StructField(\"resultado_id\", IntegerType(), False),\n",
    "    StructField(\"medalla\", StringType(), True),\n",
    "    StructField(\"deportista_id\", IntegerType(), True),\n",
    "    StructField(\"juego_id\", IntegerType(), True),\n",
    "    StructField(\"evento_id\",IntegerType(),True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA FRAME\n",
    "resultadosDF = sqlContext.read.schema(resultadosSchema) \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(path + \"resultados.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deporte.csv           deportistaError.csv   modelo_relacional.jpg\r\n",
      "deportista.csv        evento.csv            paises.csv\r\n",
      "deportista2.csv       juegos.csv            resultados.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls resources/files/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deporte\n",
    "\n",
    "La función **printschema** nos ayuda a poder ver la representación de nuestro esquema de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "deporteSchema = StructType([\n",
    "    StructField(\"deporte_id\", IntegerType(),False),\n",
    "    StructField(\"deporte\", StringType(), True) \n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "deporteDF = sqlContext.read.schema(deporteSchema) \\\n",
    "    .option(\"header\",\"true\") \\\n",
    "    .csv(path + \"deporte.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- deporte_id: integer (nullable = true)\n",
      " |-- deporte: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deporteDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- deportista_id: integer (nullable = true)\n",
      " |-- nombre: string (nullable = true)\n",
      " |-- genero: integer (nullable = true)\n",
      " |-- edad: integer (nullable = true)\n",
      " |-- altura: integer (nullable = true)\n",
      " |-- peso: float (nullable = true)\n",
      " |-- equipo_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deportistaDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Juego Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",nombre_juego,annio,temporada,ciudad\r\n",
      "1,1896 Verano,1896,Verano,Athina\r\n",
      "2,1900 Verano,1900,Verano,Paris\r\n",
      "3,1904 Verano,1904,Verano,St. Louis\r\n",
      "4,1906 Verano,1906,Verano,Athina\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 5 resources/files/juegos.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "juegoSchema = StructType([\n",
    "    StructField(\"juego_id\", IntegerType(), False),\n",
    "    StructField(\"nombre_juego\",StringType(), True ),\n",
    "    StructField(\"anio\", IntegerType(), True),\n",
    "    StructField(\"temporada\", StringType(), True),\n",
    "    StructField(\"ciudad\", StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "juegoDF  = sqlContext.read.schema(juegoSchema) \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(path + \"juegos.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renombrado de columnas\n",
    "\n",
    "Para renombrar una columna en spark lo que podemos hacer es ocupar la función **withColumnRenamed()** la cual va a recibir como primer parámetro la columna que queremos renombrar y como segundo su nombre. \n",
    "\n",
    "**Eliminar columnas:** Para eliminar columnas de nuestro data frame lo haremos mediante la función drop la cual va a recibir como parámetro ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe deportista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "deportistaDF = deportistaDF.withColumnRenamed(\"genero\", \"sexo\").drop(\"altura\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- deportista_id: integer (nullable = true)\n",
      " |-- nombre: string (nullable = true)\n",
      " |-- sexo: integer (nullable = true)\n",
      " |-- edad: integer (nullable = true)\n",
      " |-- peso: float (nullable = true)\n",
      " |-- equipo_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deportistaDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importante cuando realizamos una operación en un dataframe esta operación vuelve a cargar el data frame de nuevo es decir es una operación iterativa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operador Select\n",
    "\n",
    "el procesamiento de la funcion col es distinto ya que genera una lista envivo que va a tener todos los valores de las columnas y de esta forma hacer operaciones más pesadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "deportistaDF = deportistaDF.select(\"deportista_id\", \"nombre\",\n",
    "                    col(\"edad\").alias(\"edadAlJugar\"),\"equipo_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+-----------+---------+\n",
      "|deportista_id|             nombre|edadAlJugar|equipo_id|\n",
      "+-------------+-------------------+-----------+---------+\n",
      "|            1|          A Dijiang|         24|      199|\n",
      "|            2|           A Lamusi|         23|      199|\n",
      "|            3|Gunnar Nielsen Aaby|         24|      273|\n",
      "+-------------+-------------------+-----------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deportistaDF.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+-----------+---------+\n",
      "|deportista_id|              nombre|edadAlJugar|equipo_id|\n",
      "+-------------+--------------------+-----------+---------+\n",
      "|           54|Mohamed Jamshid A...|          0|      496|\n",
      "|           58|    Georgi Abadzhiev|          0|      154|\n",
      "|           66|     Mohamed Abakkar|          0|     1003|\n",
      "|          133|           Franz Abb|          0|      399|\n",
      "|          102|   Sayed Fahmy Abaza|          0|      308|\n",
      "|          139|George Ioannis Abbot|          0|     1043|\n",
      "|          163|     Ismail Abdallah|          0|     1095|\n",
      "|          167|Ould Lamine Abdallah|          0|      362|\n",
      "|          173| Mohamed Abdel Fatah|          0|     1003|\n",
      "|          176|Mahmoud Atter Abd...|          0|     1095|\n",
      "+-------------+--------------------+-----------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deportistaDF.sort(\"edadAlJugar\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtrado de valores en SPARK\n",
    "Para poder realizar una busqueda o un filtrado de valores lo haremos mediante una función  filter.\n",
    "Si deseo hacer un filter con diferentes sentencias lo que tendre que hacer para anidarlas será ponerlas en parentesís de la siguiente manera:\n",
    "\n",
    "dataFrame.filter((Sentencia1) & (sentencia2) & .... & (sentencian))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "deportistaDF = deportistaDF.filter((deportistaDF.edadAlJugar != 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+-----------+---------+\n",
      "|deportista_id|              nombre|edadAlJugar|equipo_id|\n",
      "+-------------+--------------------+-----------+---------+\n",
      "|            1|           A Dijiang|         24|      199|\n",
      "|            2|            A Lamusi|         23|      199|\n",
      "|            3| Gunnar Nielsen Aaby|         24|      273|\n",
      "|            4|Edgar Lindenau Aabye|         34|      278|\n",
      "|            5|Christine Jacoba ...|         21|      705|\n",
      "|            6|     Per Knut Aaland|         31|     1096|\n",
      "|            7|        John Aalberg|         31|     1096|\n",
      "|            8|Cornelia Cor Aalt...|         18|      705|\n",
      "|            9|    Antti Sami Aalto|         26|      350|\n",
      "|           10|Einar Ferdinand E...|         26|      350|\n",
      "+-------------+--------------------+-----------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deportistaDF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- equipo: string (nullable = true)\n",
      " |-- sigla: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "paiseDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Esquema de valores donde se realizará un join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- deportista_id: integer (nullable = true)\n",
      " |-- nombre: string (nullable = true)\n",
      " |-- genero: integer (nullable = true)\n",
      " |-- edad: integer (nullable = true)\n",
      " |-- altura: integer (nullable = true)\n",
      " |-- peso: float (nullable = true)\n",
      " |-- equipo_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deportistaDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- resultado_id: integer (nullable = true)\n",
      " |-- medalla: string (nullable = true)\n",
      " |-- deportista_id: integer (nullable = true)\n",
      " |-- juego_id: integer (nullable = true)\n",
      " |-- evento_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resultadosDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- juego_id: integer (nullable = true)\n",
      " |-- nombre_juego: string (nullable = true)\n",
      " |-- anio: integer (nullable = true)\n",
      " |-- temporada: string (nullable = true)\n",
      " |-- ciudad: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "juegoDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deporte.csv           deportistaError.csv   modelo_relacional.jpg\r\n",
      "deportista.csv        evento.csv            paises.csv\r\n",
      "deportista2.csv       juegos.csv            resultados.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls resources/files/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evento_id,evento,deporte_id\r\n",
      "1,Basketball Men's Basketball,1\r\n",
      "2,Judo Men's Extra-Lightweight,2\r\n",
      "3,Football Men's Football,3\r\n",
      "4,Tug-Of-War Men's Tug-Of-War,4\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 5 resources/files/evento.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "deportesOlimpicosSchema = StructType([\n",
    "    StructField(\"evento_id\", IntegerType(), True),\n",
    "    StructField(\"nombre\", StringType(), False),\n",
    "    StructField(\"deporte_id\", IntegerType(),True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "deportesOlimpicosDF = sqlContext.read.schema(deportesOlimpicosSchema)\\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(path + \"evento.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+----------+\n",
      "|evento_id|              nombre|deporte_id|\n",
      "+---------+--------------------+----------+\n",
      "|        1|Basketball Men's ...|         1|\n",
      "|        2|Judo Men's Extra-...|         2|\n",
      "|        3|Football Men's Fo...|         3|\n",
      "|        4|Tug-Of-War Men's ...|         4|\n",
      "|        5|Speed Skating Wom...|         5|\n",
      "|        6|Speed Skating Wom...|         5|\n",
      "|        7|Cross Country Ski...|         6|\n",
      "|        8|Cross Country Ski...|         6|\n",
      "|        9|Cross Country Ski...|         6|\n",
      "|       10|Cross Country Ski...|         6|\n",
      "+---------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deportesOlimpicosDF.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join  en Spark\n",
    "Para realizar el Join lo haremos mediante la función join y a la cual le tendremos que pasar lo siguiente parámetros:\n",
    "- La tabla con la cual se va a cruzar \n",
    "- el tipo de join que se va a realizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+-------+-------------+--------------------+\n",
      "|              nombre|edad_al_jugar|medalla|anio_de_juego|   nombre_disciplina|\n",
      "+--------------------+-------------+-------+-------------+--------------------+\n",
      "|           A Dijiang|           24|     NA|         1992|Basketball Men's ...|\n",
      "|            A Lamusi|           23|     NA|         2012|Judo Men's Extra-...|\n",
      "| Gunnar Nielsen Aaby|           24|     NA|         1920|Football Men's Fo...|\n",
      "|Edgar Lindenau Aabye|           34|   Gold|         1900|Tug-Of-War Men's ...|\n",
      "|Christine Jacoba ...|           21|     NA|         1994|Speed Skating Wom...|\n",
      "|Christine Jacoba ...|           21|     NA|         1994|Speed Skating Wom...|\n",
      "|Christine Jacoba ...|           21|     NA|         1992|Speed Skating Wom...|\n",
      "|Christine Jacoba ...|           21|     NA|         1992|Speed Skating Wom...|\n",
      "|Christine Jacoba ...|           21|     NA|         1988|Speed Skating Wom...|\n",
      "|Christine Jacoba ...|           21|     NA|         1988|Speed Skating Wom...|\n",
      "+--------------------+-------------+-------+-------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deportistaDF.join(resultadosDF,deportistaDF.deportista_id == resultadosDF.deportista_id, \"left\") \\\n",
    "            .join(juegoDF,juegoDF.juego_id == resultadosDF.juego_id, \"left\") \\\n",
    "            .join(deportesOlimpicosDF, deportesOlimpicosDF.evento_id == resultadosDF.evento_id, \"left\") \\\n",
    "            .select(deportistaDF.nombre, col(\"edad\").alias(\"edad_al_jugar\"),\n",
    "                    \"medalla\", col(\"anio\").alias(\"anio_de_juego\"),\n",
    "                   deportesOlimpicosDF.nombre.alias(\"nombre_disciplina\")).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importante\n",
    "Cuando nosotros escribimos un join lo que hacemos es ponerlo en una sola linea ya que de otro modo  no corre nuestro programa.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-----+\n",
      "|Medalla|  equipo|sigla|\n",
      "+-------+--------+-----+\n",
      "| Silver|Zimbabwe|  ZIM|\n",
      "| Bronze|Zimbabwe|  ZIM|\n",
      "| Silver|Zimbabwe|  ZIM|\n",
      "|   Gold|Zimbabwe|  ZIM|\n",
      "|   Gold|Zimbabwe|  ZIM|\n",
      "| Silver|Zimbabwe|  ZIM|\n",
      "|   Gold|Zimbabwe|  ZIM|\n",
      "|   Gold|Zimbabwe|  ZIM|\n",
      "|   Gold|Zimbabwe|  ZIM|\n",
      "| Silver|Zimbabwe|  ZIM|\n",
      "+-------+--------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resultadosDF.filter(resultadosDF.medalla != \"NA\") \\\n",
    "    .join(deportistaDF, resultadosDF.deportista_id == deportistaDF.deportista_id, \"left\") \\\n",
    "    .join(paisesDF, paisesDF.id == deportistaDF.equipo_id, \"left\") \\\n",
    "    .select(\"Medalla\",\"equipo\",\"sigla\") \\\n",
    "    .sort(col(\"sigla\").desc()).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "medallistaXAnio = deportistaDF \\\n",
    "    .join(resultadosDF, deportistaDF.deportista_id == resultadosDF.deportista_id, \"left\") \\\n",
    "    .join(juegoDF, juegoDF.juego_id == resultadosDF.juego_id,\"left\") \\\n",
    "    .join(paisesDF, paisesDF.id == deportistaDF.equipo_id, \"left\" ) \\\n",
    "    .join(deportesOlimpicosDF, deportesOlimpicosDF.evento_id == resultadosDF.evento_id, \"left\") \\\n",
    "    .join(deporteDF, deporteDF.deporte_id == deportesOlimpicosDF.deporte_id, \"left\") \\\n",
    "    .select( \"sigla\",\n",
    "            \"anio\",\n",
    "            \"medalla\",\n",
    "           deportesOlimpicosDF.nombre.alias(\"nombre_subdisciplina\"),\n",
    "           deporteDF.deporte.alias(\"nombre_disciplina\"),\n",
    "           deportistaDF.nombre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-------+--------------------+-----------------+--------------------+\n",
      "|sigla|anio|medalla|nombre_subdisciplina|nombre_disciplina|              nombre|\n",
      "+-----+----+-------+--------------------+-----------------+--------------------+\n",
      "|  CHN|1992|     NA|Basketball Men's ...|       Basketball|           A Dijiang|\n",
      "|  CHN|2012|     NA|Judo Men's Extra-...|             Judo|            A Lamusi|\n",
      "|  DEN|1920|     NA|Football Men's Fo...|         Football| Gunnar Nielsen Aaby|\n",
      "|  SWE|1900|   Gold|Tug-Of-War Men's ...|       Tug-Of-War|Edgar Lindenau Aabye|\n",
      "|  NED|1994|     NA|Speed Skating Wom...|    Speed Skating|Christine Jacoba ...|\n",
      "|  NED|1994|     NA|Speed Skating Wom...|    Speed Skating|Christine Jacoba ...|\n",
      "|  NED|1992|     NA|Speed Skating Wom...|    Speed Skating|Christine Jacoba ...|\n",
      "|  NED|1992|     NA|Speed Skating Wom...|    Speed Skating|Christine Jacoba ...|\n",
      "|  NED|1988|     NA|Speed Skating Wom...|    Speed Skating|Christine Jacoba ...|\n",
      "|  NED|1988|     NA|Speed Skating Wom...|    Speed Skating|Christine Jacoba ...|\n",
      "+-----+----+-------+--------------------+-----------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "medallistaXAnio.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Operador Group By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "medallistaXanio2 = medallistaXAnio.filter(medallistaXAnio.medalla !=  \"NA\") \\\n",
    "    .sort(\"anio\") \\\n",
    "    .groupBy(\"sigla\", \"anio\", \"nombre_subdisciplina\") \\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sigla: string (nullable = true)\n",
      " |-- anio: integer (nullable = true)\n",
      " |-- nombre_subdisciplina: string (nullable = true)\n",
      " |-- count: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "medallistaXanio2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segun la documentación de spark la mejor forma de realizar un agrupación es mediante el operador **agg** o también llamada  agregación la cual nos va a permit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----------------+------------------+\n",
      "|sigla|anio|total_de_medallas| medallas_promedio|\n",
      "+-----+----+-----------------+------------------+\n",
      "|  USA|2012|              121|1.9836065573770492|\n",
      "|  FRA|2006|               12|1.3333333333333333|\n",
      "|  BLR|2000|                9|               1.8|\n",
      "|  FIN|1988|               10|               2.5|\n",
      "|  KOR|2010|                3|               1.5|\n",
      "|  FRA|1948|               52|              2.08|\n",
      "|  GBR|2000|               30|1.5789473684210527|\n",
      "|  QAT|2012|                2|               1.0|\n",
      "|  JPN|1932|               11|               2.2|\n",
      "|  FRG|1994|                2|               1.0|\n",
      "|  NED|1972|                7|               1.4|\n",
      "|  GER|1932|               35|1.8421052631578947|\n",
      "|  NZL|1988|               11|             1.375|\n",
      "|  AUS|1972|               13|1.1818181818181819|\n",
      "|  BAH|2008|                2|               2.0|\n",
      "|  SWE|1968|               11|             1.375|\n",
      "|  KOR|1988|               43|2.0476190476190474|\n",
      "|  IRL|1948|                1|               1.0|\n",
      "|  IND|1928|                7|               7.0|\n",
      "|  YUG|1976|               10|3.3333333333333335|\n",
      "+-----+----+-----------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "medallistaXanio2.groupBy(\"sigla\", \"anio\") \\\n",
    "    .agg(sum(\"count\").alias(\"total_de_medallas\"), avg(\"count\").alias(\"medallas_promedio\")) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL en SPARK\n",
    "\n",
    "Para poder realizar operacione de tipo **SQL** en SPARK lo que tendremos que hacer es crear resgistro temporales esto lo haremos mediante la función **registerTempTable()** la cual va a recibir como parámetro el nombre de nuestra tabla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultadosDF.registerTempTable(\"resultado\")\n",
    "deportistaDF.registerTempTable(\"deportista\")\n",
    "paisesDF.registerTempTable(\"paises\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+------+----+------+----+---------+\n",
      "|deportista_id|              nombre|genero|edad|altura|peso|equipo_id|\n",
      "+-------------+--------------------+------+----+------+----+---------+\n",
      "|            1|           A Dijiang|     1|  24|   180|80.0|      199|\n",
      "|            2|            A Lamusi|     1|  23|   170|60.0|      199|\n",
      "|            3| Gunnar Nielsen Aaby|     1|  24|     0| 0.0|      273|\n",
      "|            4|Edgar Lindenau Aabye|     1|  34|     0| 0.0|      278|\n",
      "|            5|Christine Jacoba ...|     2|  21|   185|82.0|      705|\n",
      "|            6|     Per Knut Aaland|     1|  31|   188|75.0|     1096|\n",
      "|            7|        John Aalberg|     1|  31|   183|72.0|     1096|\n",
      "|            8|Cornelia Cor Aalt...|     2|  18|   168| 0.0|      705|\n",
      "|            9|    Antti Sami Aalto|     1|  26|   186|96.0|      350|\n",
      "|           10|Einar Ferdinand E...|     1|  26|     0| 0.0|      350|\n",
      "+-------------+--------------------+------+----+------+----+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"SELECT * FROM deportista\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- equipo: string (nullable = true)\n",
      " |-- sigla: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "paisesDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-----+\n",
      "|medalla|  equipo|sigla|\n",
      "+-------+--------+-----+\n",
      "|   Gold|Zimbabwe|  ZIM|\n",
      "|   Gold|Zimbabwe|  ZIM|\n",
      "|   Gold|Zimbabwe|  ZIM|\n",
      "| Silver|Zimbabwe|  ZIM|\n",
      "|   Gold|Zimbabwe|  ZIM|\n",
      "| Bronze|Zimbabwe|  ZIM|\n",
      "| Silver|Zimbabwe|  ZIM|\n",
      "|   Gold|Zimbabwe|  ZIM|\n",
      "| Silver|Zimbabwe|  ZIM|\n",
      "| Silver|Zimbabwe|  ZIM|\n",
      "+-------+--------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "         SELECT medalla, equipo,sigla FROM resultado r\n",
    "         JOIN deportista d\n",
    "         ON r.deportista_id = d.deportista_id\n",
    "         JOIN paises p \n",
    "         ON p.id = d.equipo_id \n",
    "         WHERE medalla <> \"NA\"\n",
    "         ORDER BY sigla DESC\n",
    "        \"\"\"\n",
    "\n",
    "sqlContext.sql(query).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL vs Data Frames \n",
    "Algo que no puede surgir como duda es , ¿Porque no manejamos como **SQL** todos nuestros dataframes? y la respuesta es que es más lento en comparación al manejo que se haría ocupando  Spark, por lo que es mejor manjearlo de esta manera."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Las funciones definidas por el usuario o UDF, \n",
    "por sus siglas en inglés, son una funcionalidad agregada en Spark para definir funciones basadas en columnas las cuales permiten extender las capacidades de Spark al momento de transformar el set de datos.\n",
    "\n",
    "- Este tipo de implementaciones son convenientes cuando tenemos un desarrollo extenso donde hemos identificado la periodicidad de tareas repetitivas como suele ser en pasos de limpieza de datos, transformación o renombrado dinámico de columnas.\n",
    "\n",
    "- Por lo anterior es común encontrar en un proyecto de Spark una librería independiente donde existen todas estas funciones agregadas para que los desarrolladores involucrados en el proyecto puedan usarlas a conveniencia.\n",
    "\n",
    "- El uso de UDF no implica que las funciones que podemos crear nativamente con Python, Scala, R o Java no sean útiles. Una UDF tiene el objetivo de ofrecer un estándar interno en el proyecto que nos encontremos realizando. Además, en caso de ser necesario, una UDF puede ser modificada con ayuda de decoradores para que sea más extensible en diversos escenarios a los cuales nos podemos enfrentar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deportista_id,nombre,genero,edad,altura,peso,equipo_id\r\n",
      "1,A Dijiang,1,24,180,80,199\r\n",
      "2,A Lamusi,1,23,170,60,199\r\n",
      "3,Gunnar Nielsen Aaby,1,24,,,273\r\n",
      "4,Edgar Lindenau Aabye,1,34,,,278\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 5 resources/files/deportistaError.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "deportistaError = sc.textFile( path + \"deportistaError.csv\") \\\n",
    "    .map(lambda l : l.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['deportista_id', 'nombre', 'genero', 'edad', 'altura', 'peso', 'equipo_id'],\n",
       " ['1', 'A Dijiang', '1', '24', '180', '80', '199']]"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deportistaError.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminaEncabezado(index, iterator):\n",
    "    return iter(list(iterator)[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "deportistaError = deportistaError.mapPartitionsWithIndex(eliminaEncabezado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1', 'A Dijiang', '1', '24', '180', '80', '199'],\n",
       " ['2', 'A Lamusi', '1', '23', '170', '60', '199'],\n",
       " ['3', 'Gunnar Nielsen Aaby', '1', '24', '', '', '273'],\n",
       " ['4', 'Edgar Lindenau Aabye', '1', '34', '', '', '278'],\n",
       " ['5', 'Christine Jacoba Aaftink', '2', '21', '185', '82', '705']]"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "deportistaError.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Mapeo del archivo\n",
    "deportistaError = deportistaError.map(lambda l : (l[0],l[1],l[2],l[3],l[4],l[5],l[6]))\n",
    "\n",
    "## Estructura que vamos a ocupar para el data frame\n",
    "deportistaErrorSchema = StructType([\n",
    "    StructField(\"deportista_id\", StringType(), False),\n",
    "    StructField(\"nombre\", StringType(),False),\n",
    "    StructField(\"genero\",StringType(),False),\n",
    "    StructField(\"edad\",StringType(),False ),\n",
    "    StructField(\"altura\",StringType(), False),\n",
    "    StructField(\"peso\",StringType() , False),\n",
    "    StructField(\"equipo_id\", StringType(), False)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "deportistaErrorDF = sqlContext.createDataFrame(deportistaError, deportistaErrorSchema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición de función UDF\n",
    "\n",
    "Para hacer una función que se aplique a nuestros dataframes lo que haremos en spark será lo siguiente:\n",
    "- Definir la función que vamos a ocupar.\n",
    "- Pasar la función a formato UDF.\n",
    "- Dar de alta la función en formato UDF, para hacer esto tendremos que pasar la función register dos parámetros :\n",
    "    - Nombre de función como será identificada por spark\n",
    "    - La función en formato UDF \n",
    "    \n",
    "de esta forma declaramos nuestras funciones nativas en spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(z)>"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "def conversionEnteros(valor):\n",
    "    return int(valor) if len(valor) > 0 else None\n",
    "\n",
    "# definimos la funcion en un formato udf\n",
    "conversionEnteros_udf = udf(lambda z: conversionEnteros(z), IntegerType())\n",
    "# Damos de alta nuestra funcion como de tipo udf\n",
    "sqlContext.udf.register(\"converisionEnteros_udf\", conversionEnteros_udf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|alturaUDF|\n",
      "+---------+\n",
      "|      180|\n",
      "|      170|\n",
      "|     null|\n",
      "|     null|\n",
      "|      185|\n",
      "|      188|\n",
      "|      183|\n",
      "|      168|\n",
      "|      186|\n",
      "|     null|\n",
      "|      182|\n",
      "|      172|\n",
      "|      159|\n",
      "|      171|\n",
      "|     null|\n",
      "|      184|\n",
      "|      175|\n",
      "|      189|\n",
      "|     null|\n",
      "|      176|\n",
      "+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deportistaErrorDF.select(conversionEnteros_udf(\"altura\") \\\n",
    "                        .alias(\"alturaUDF\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Particionado de da datos\n",
    "\n",
    "- Como se ha descrito en clases pasadas, los RDD son la capa de abstracción primaria para poder interactuar con los datos que viven en nuestro ambiente de Spark. Aunque estos puedan ser enmascarados con un esquema dotándolos de las facultades propias de los DataFrames, la información de fondo sigue operando como RDD.\n",
    "\n",
    "- Por lo tanto, la información, como indica el nombre de los RDD, se maneja de forma distribuida a lo largo del clúster, facilitando las operaciones que se van a ejecutar, ya que segmentos de información pueden encontrarse en diferentes ejecutores reduciendo el tiempo necesario para acceder a la información y poder así realizar los cálculos necesarios.\n",
    "\n",
    "- Cuando un RDD o Dataframe es creado, según las especificaciones que se indiquen a la aplicación de Spark, creará un esquema de particionado básico, el cual distribuirá los datos a lo largo del clúster. Siendo así que al momento de ejecutar una acción, esta se ejecutará entre los diversos fragmentos de información que existan para poder así realizar de la forma más rápida las operaciones. Es por eso que un correcto esquema de particionado es clave para poder tener aplicaciones rápidas y precisas que además consuman pocos recursos de red.\n",
    "\n",
    "- Otra de las tareas fundamentales es la replicación de componentes y sus fragmentos, ya que al aumentar la disponibilidad de estos podremos asegurar una tolerancia a fallos, mientras más se replique un valor es más probable que no se pierda si existe un fallo de red o energía, además de permitir una disponibilidad casi inmediata del archivo buscado.\n",
    "\n",
    "- La partición y replicación son elementos que deben ser analizados según el tipo de negocio o requerimientos que se tengan en el desarrollo que se encuentre en progreso, por lo cual la cantidad de datos replicados o granularidad de datos existentes en los fragmentos dependerá en función de las reglas de negocio.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persistencia\n",
    " Problemas al usar un RDD o DF varias veces:\n",
    " - Al ser spark de ejecución perezosa recomputa  su componente y sus dependencias cada vez que se ejecuta una acción.\n",
    " - Es costoso (Especialmente en problemas iterativos.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solución : \n",
    "Una solución para eso es lo siguiente:\n",
    "- Conservar el componente en memorio y/o disco.\n",
    "- Métodos **cache()** o **persist()** nos ayudan.\n",
    "- En PySpark los datos son almacenados de forma serializada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.storagelevel import StorageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medallistaXAnio.is_cached"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. El primer paso en la persistencia de los datos en spark será mandar nuestro RDD a memoria cache de la siguiente forma :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[541] at javaToPython at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medallistaXAnio.rdd.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Para poder ver en que nivel de cache se encuentra nuestro rdd lo haremos con la función **getStorageLevel()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StorageLevel(False, True, False, False, 1)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medallistaXAnio.rdd.getStorageLevel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si deseamos interpretar lo anterior lo podemos hacer mediante el la documentación [doc](https://spark.apache.org/docs/2.4.6/api/python/pyspark.html#pyspark.StorageLevel)\n",
    "en resumen el primer true es  para el nivel de disco, el segundo true es para el nivel de cache y el tercer true es para el uso de un heap.\n",
    "\n",
    "Lo ideal sería que lo tuvieramos en 3 fuentes diferentes para evitar perdida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para aplicar la persistencia en un nivel dado lo haremos mediante la función **persist()** la cual va a recibir el nivel de memoria que deseo almacenar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[541] at javaToPython at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medallistaXAnio.rdd.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo anterior se realizo ya que al ingresar una persistencia nueva en los datos será necesrio que no tenga una agregada ya que si pasa esto lo que hará spark es mandarnos un error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[541] at javaToPython at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medallistaXAnio.rdd.persist(StorageLevel.MEMORY_AND_DISK_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez hecho esto lo que sucederá es que nuestro RDD estará persistido en memoria y en disco además de replicado dos veces.\n",
    "El replicado será sobre el  particionamiento que realiza SPARK.\n",
    "\n",
    "Pero siguiendo la documentación de spark que nos dice que debe haber una persistencia de dos niveles y particionado de 3 lo que haremos será  aplicar de nuevo el método **unpersist()** y posteriormente a esto definir el nuevo particionado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[541] at javaToPython at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medallistaXAnio.rdd.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición de nuestro propio particionado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "StorageLevel.MEMORY_AND_DISK_3 = StorageLevel(True, True, False, False, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[541] at javaToPython at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medallistaXAnio.rdd.persist(StorageLevel.MEMORY_AND_DISK_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Particionado de datos \n",
    "Básicamente cuantos lacayos van a ejecutar el trabajo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
